{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T03:22:56.124398Z",
     "iopub.status.busy": "2024-04-18T03:22:56.124036Z",
     "iopub.status.idle": "2024-04-18T03:23:16.713384Z",
     "shell.execute_reply": "2024-04-18T03:23:16.712473Z",
     "shell.execute_reply.started": "2024-04-18T03:22:56.124369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.10/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe, vocab\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hierarchical Attention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Related Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T03:26:42.952900Z",
     "iopub.status.busy": "2024-04-18T03:26:42.951696Z",
     "iopub.status.idle": "2024-04-18T03:26:42.974754Z",
     "shell.execute_reply": "2024-04-18T03:26:42.973703Z",
     "shell.execute_reply.started": "2024-04-18T03:26:42.952858Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataPreprocessorHcl():\n",
    "\n",
    "    def __init__(self, num_classes, data_vocab):\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab = data_vocab\n",
    "        print(\"Vocab created: {} unique tokens\".format(len(self.vocab)))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained_embeds(cls, num_classes, embed_path, embed_dim, sep=\" \",  specials=['<unk>']):\n",
    "        # start with all '0's for special tokens\n",
    "        embeds = [np.asarray([0]*embed_dim, dtype=np.float32)]*len(specials)\n",
    "        words = OrderedDict()\n",
    "        with open(embed_path, encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                #twitter 27B not used, ignore this if block\n",
    "                if i == 38522 and 'twitter.27B.100d' in embed_path:\n",
    "                    continue\n",
    "                splitline = line.split()\n",
    "                \n",
    "                word = splitline[0]\n",
    "                if word not in words:\n",
    "                    words[word] = 0\n",
    "                words[word]+=1\n",
    "                embeds.append(np.asarray(splitline[1:], dtype=np.float32))\n",
    "                \n",
    "        embeds = torch.tensor(np.array(embeds))\n",
    "        data_vocab = vocab(words, specials=specials)\n",
    "        data_vocab.set_default_index(data_vocab['<unk>'])\n",
    "        return cls(num_classes, data_vocab), embeds\n",
    "\n",
    "    @classmethod\n",
    "    def __yield_tokens(cls, df):\n",
    "        for row in df.itertuples(index=False):\n",
    "            yield word_tokenize(row.Text.lower())\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def preprocess_data(self, df, max_sent_len, max_num_sents, clean=True):\n",
    "        '''\n",
    "        Converts text into integers that index the vocab, and labels into the range [0,num_classes-1].\n",
    "        Also calculates the number of sentences in each text and the length (number of tokens) of each sentence.\n",
    "        \n",
    "        Returns\n",
    "            X: an array of size (N, max_sent_len, max_num_sents) containing the indices and padding\n",
    "            ylens: a df where each row has the processed label, number of sentences and a list of sentence lengths\n",
    "        '''\n",
    "        if clean:\n",
    "            X = df['Text'].apply(lambda t: [self.vocab(word_tokenize(s.lower())) for s in sent_tokenize(t.replace(\"'\",\"\"))])\n",
    "        else:\n",
    "            X = df['Text'].apply(lambda t: [self.vocab(word_tokenize(s.lower())) for s in sent_tokenize(t)])\n",
    "        num_sentences = X.apply(lambda sentences : min(max_num_sents, len(sentences)))\n",
    "        num_sentences.name = 'Num_Sentences'\n",
    "        num_tokens = X.apply(lambda sentences : list(map(lambda s: min(max_sent_len, len(s)), sentences))[:max_num_sents])\n",
    "        num_tokens = num_tokens.apply(lambda num_ls : num_ls + [0 for _ in range(max_num_sents-len(num_ls))]) #padding\n",
    "        num_tokens.name = 'Num_Tokens'\n",
    "        \n",
    "        X_padded = np.zeros((len(X), max_num_sents, max_sent_len), dtype='int32')\n",
    "        for i, sentences in X.items():\n",
    "            for j, sent in enumerate(sentences):\n",
    "                if j >= max_num_sents:\n",
    "                    break\n",
    "                k = min(max_sent_len, len(sent))\n",
    "                X_padded[i,j,:k] = sent[:k]\n",
    "                \n",
    "        y = df['Label'].apply(lambda l: l-1)\n",
    "        return X_padded, pd.concat([y, num_sentences, num_tokens], axis=1)\n",
    "\n",
    "class WrapperDatasetHcl(Dataset):\n",
    "    '''\n",
    "    Wrapper for use with dataloader\n",
    "    '''\n",
    "    def __init__(self, X, y_and_lens):\n",
    "        self.X = X\n",
    "        self.y = y_and_lens['Label']\n",
    "        self.num_sents = y_and_lens['Num_Sentences']\n",
    "        self.num_tokens = y_and_lens['Num_Tokens']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y.iloc[idx], self.num_sents.iloc[idx],  self.num_tokens.iloc[idx]\n",
    "\n",
    "def collate_fnHcl(batch):\n",
    "    X = []\n",
    "    y = []\n",
    "    num_sent = []\n",
    "    sent_len = []\n",
    "    for row in batch:\n",
    "        X.append(row[0])\n",
    "        y.append(row[1])\n",
    "        num_sent.append(row[2])\n",
    "        sent_len.append(row[3])\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    num_sent = torch.tensor(num_sent, dtype=torch.long)\n",
    "    sent_len = torch.tensor(sent_len, dtype=torch.long)\n",
    "    return X, y, num_sent, sent_len\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T06:39:31.630265Z",
     "iopub.status.busy": "2024-04-16T06:39:31.629552Z",
     "iopub.status.idle": "2024-04-16T06:39:31.672307Z",
     "shell.execute_reply": "2024-04-16T06:39:31.671366Z",
     "shell.execute_reply.started": "2024-04-16T06:39:31.630229Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, train_loader, val_loader, num_epochs, lr, weight_decay = 0,\n",
    "               lr_anneal_factor=None, lr_anneal_patience=None,\n",
    "               loss_weights=None,\n",
    "               save_loss_acc_plots=True):\n",
    "        #Data\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        #Model\n",
    "        self.model=model.to(DEVICE)\n",
    "\n",
    "        #Training\n",
    "        self.num_epochs = num_epochs\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        if loss_weights is not None:\n",
    "            self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(loss_weights, dtype=torch.float32, device=DEVICE))\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.to_anneal_lr = lr_anneal_factor and lr_anneal_patience\n",
    "        if self.to_anneal_lr:\n",
    "            self.scheduler = ReduceLROnPlateau(self.optimizer,\n",
    "                                             factor=lr_anneal_factor,\n",
    "                                             patience=lr_anneal_patience)\n",
    "\n",
    "    def train(self, save_folder_root_path, file_name_root,\n",
    "            plot_loss_acc=True, verbose=True):\n",
    "        #account for pytorch versions < 2.2\n",
    "        if hasattr(self.scheduler, 'verbose'):\n",
    "            self.scheduler.verbose=verbose\n",
    "            \n",
    "        #Set up logging\n",
    "        self.__set_up_logging(save_folder_root_path, file_name_root, plot_loss_acc)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(1, self.num_epochs+1):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            preds = []\n",
    "            truths = []\n",
    "            for X, y, num_sent, sent_len in tqdm(self.train_loader, disable=not verbose):\n",
    "                #Move to correct device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                #Forward pass\n",
    "                outputs = self.model(X, num_sent, sent_len, return_attn_weights=False)\n",
    "                loss = self.loss_fn(outputs, y)\n",
    "\n",
    "                #Backprop\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                #Logging purposes\n",
    "                train_loss.append(loss.item())\n",
    "                preds.append(torch.argmax(outputs, dim=-1).cpu())\n",
    "                truths.append(y.cpu())\n",
    "\n",
    "            #Validation\n",
    "            self.model.eval()\n",
    "            val_loss, val_acc, val_f1 = self.validate()\n",
    "\n",
    "            #Logging\n",
    "            train_loss, train_acc, train_f1 = self.__log(train_loss, preds, truths, val_loss, val_acc, val_f1)\n",
    "            tqdm.write(\"Epoch {} Complete:\\n Train: loss={}, acc={}, F1={}\\n Val  : loss={}, acc={}, F1={}\\n\".format(\n",
    "                        i, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1))\n",
    "            \n",
    "            #Save model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.model.state_dict(),\n",
    "                           self.model_folder + file_name_root + '_model.pt')\n",
    "            \n",
    "            #LR Annealing\n",
    "            if self.to_anneal_lr:\n",
    "                #account for pytorch versions >= 2.2\n",
    "                if not hasattr(self.scheduler, 'verbose') and i > 1:\n",
    "                    last_lr = self.scheduler.get_last_lr()\n",
    "                    \n",
    "                self.scheduler.step(val_loss)\n",
    "                \n",
    "                #account for pytorch versions >= 2.2\n",
    "                if not hasattr(self.scheduler, 'verbose') and i > 1 and last_lr != self.scheduler.get_last_lr():\n",
    "                    print(\"LR Reduced from {} to {} for next epoch onwards\".format(last_lr, self.scheduler.get_last_lr()))\n",
    "                    \n",
    "        #Save plots\n",
    "        if self.plot_loss_acc:\n",
    "            self.plot_metrics()\n",
    "\n",
    "        return self.model_folder + file_name_root + '_model.pt', self.plots_folder + file_name_root + '_plot.png'\n",
    "\n",
    "    def validate(self):\n",
    "        with torch.no_grad():\n",
    "            losses = []\n",
    "            preds = []\n",
    "            truths = []\n",
    "            for X, y, num_sent, sent_len in self.val_loader:\n",
    "                #Move to correct device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                #Forward pass\n",
    "                outputs = self.model(X, num_sent, sent_len, return_attn_weights=False)\n",
    "                loss = self.loss_fn(outputs, y)\n",
    "\n",
    "                #Logging\n",
    "                losses.append(loss.item())\n",
    "                preds.append(torch.argmax(outputs, dim=-1).cpu())\n",
    "                truths.append(y.cpu())\n",
    "        preds = torch.cat(preds)\n",
    "        truths = torch.cat(truths)\n",
    "        return sum(losses)/len(losses), accuracy_score(truths, preds), f1_score(truths, preds, average='macro')\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "        axs[0].plot(range(1, self.num_epochs + 1), self.train_metrics['loss'], color='b', label='Train')\n",
    "        axs[0].plot(range(1, self.num_epochs + 1), self.val_metrics['loss'], color='r', label='Validation')\n",
    "        axs[0].set_ylabel('Loss')\n",
    "        axs[0].set_xlabel('Epochs')\n",
    "        axs[0].set_ylim(bottom=0)\n",
    "        axs[0].set_xticks(range(0, self.num_epochs + 1, 2))\n",
    "        axs[0].grid(visible=True, which='major', axis='both')\n",
    "\n",
    "        axs[1].plot(range(1, self.num_epochs + 1), self.train_metrics['accuracy'], color='b', label='Train')\n",
    "        axs[1].plot(range(1, self.num_epochs + 1), self.val_metrics['accuracy'], color='r', label='Validation')\n",
    "        axs[1].set_ylabel('Accuracy')\n",
    "        axs[1].set_xlabel('Epochs')\n",
    "        axs[1].set_ylim(bottom=0)\n",
    "        axs[1].set_xticks(range(0, self.num_epochs + 1, 2))\n",
    "        axs[1].grid(visible=True, which='major', axis='both')\n",
    "\n",
    "        axs[2].plot(range(1, self.num_epochs + 1), self.train_metrics['f1'], color='b', label='Train')\n",
    "        axs[2].plot(range(1, self.num_epochs + 1), self.val_metrics['f1'], color='r', label='Validation')\n",
    "        axs[2].set_ylabel('F1 (Macro-averaged)')\n",
    "        axs[2].set_xlabel('Epochs')\n",
    "        axs[2].set_ylim(bottom=0)\n",
    "        axs[2].set_xticks(range(0, self.num_epochs + 1, 2))\n",
    "        axs[2].grid(visible=True, which='major', axis='both')\n",
    "\n",
    "        fig.legend(*axs[2].get_legend_handles_labels(), loc='upper center')\n",
    "        fig.savefig(self.plots_folder + self.file_name_root + '_plot.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def __set_up_logging(self, save_folder_root_path, file_name_root, plot_loss_acc):\n",
    "        self.model_folder = save_folder_root_path+'model/'\n",
    "        self.plots_folder = save_folder_root_path+'plots/'\n",
    "        self.file_name_root = file_name_root\n",
    "        self.plot_loss_acc = plot_loss_acc\n",
    "        if not os.path.exists(self.model_folder):\n",
    "            os.makedirs(self.model_folder)\n",
    "        if self.plot_loss_acc and not os.path.exists(self.plots_folder):\n",
    "            os.makedirs(self.plots_folder)\n",
    "        self.train_metrics = {'loss':[], 'accuracy':[], 'f1':[]}\n",
    "        self.val_metrics = {'loss':[], 'accuracy':[], 'f1':[]}\n",
    "        return\n",
    "    \n",
    "    def __log(self, train_loss, train_preds, train_truths, val_loss, val_acc, val_f1):\n",
    "        train_loss = sum(train_loss)/len(train_loss)\n",
    "        preds = torch.cat(train_preds)\n",
    "        truths = torch.cat(train_truths)\n",
    "        train_acc = accuracy_score(truths, preds)\n",
    "        train_f1 = f1_score(truths, preds, average='macro')\n",
    "\n",
    "        self.train_metrics['loss'].append(train_loss)\n",
    "        self.train_metrics['accuracy'].append(train_acc)\n",
    "        self.train_metrics['f1'].append(train_f1)\n",
    "        self.val_metrics['loss'].append(val_loss)\n",
    "        self.val_metrics['accuracy'].append(val_acc)\n",
    "        self.val_metrics['f1'].append(val_f1)\n",
    "\n",
    "        return train_loss, train_acc, train_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T06:38:33.830356Z",
     "iopub.status.busy": "2024-04-16T06:38:33.829902Z",
     "iopub.status.idle": "2024-04-16T06:38:33.856108Z",
     "shell.execute_reply": "2024-04-16T06:38:33.854847Z",
     "shell.execute_reply.started": "2024-04-16T06:38:33.830325Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionUnit(nn.Module):\n",
    "    def __init__(self, input_dim,, num_outputs=1, attn_dropout=0.0):\n",
    "        super(AttentionUnit, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, input_dim)\n",
    "        self.query = nn.Linear(input_dim, num_outputs, bias=False)\n",
    "        \n",
    "    def forward(self, encoder_output, padding_positions=None, return_weights=False):\n",
    "        # [B,L,H]-->[B,L,H]\n",
    "        hidden_rep = F.tanh(self.hidden(encoder_output))\n",
    "        \n",
    "        # [B,L,H]-->[B,L,1]\n",
    "        similarity = self.query(hidden_rep)\n",
    "        if padding_positions is not None:\n",
    "            similarity = similarity.masked_fill(padding_positions, -float('inf'))\n",
    "        attention_weights = F.softmax(similarity, dim=1)\n",
    "        \n",
    "        #Return weighted sum [B,L,1], [B,L,H]-->[B,H]\n",
    "        if return_weights:\n",
    "            return torch.bmm(attention_weights.transpose(1,2), hidden_rep).squeeze(1), attention_weights\n",
    "        return torch.bmm(attention_weights.transpose(1,2), hidden_rep).squeeze(1)\n",
    "\n",
    "class BiLSTMHeAttFCNNClassifier(nn.Module):\n",
    "    '''\n",
    "    Classifier that uses heirarchical attention to encode a document and \n",
    "    a Fully-Connected Neural Network(FCNN) as a decoder.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, vocab_len, embed_dim, hidden_dim, num_lstm_layers, num_classes, attn_dropout=0.0, pretrained_embeddings=None, freeze_embeds=False):\n",
    "        super(BiLSTMHeAttFCNNClassifier, self).__init__()\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=freeze_embeds)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_len, embedding_dim=embed_dim)\n",
    "        \n",
    "        self.word_encoder = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=num_lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.word_attn = AttentionUnit(2*hidden_dim)\n",
    "        \n",
    "        self.sent_encoder = nn.LSTM(input_size=2*hidden_dim, hidden_size=hidden_dim, num_layers=num_lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.sent_attn = AttentionUnit(2*hidden_dim)\n",
    "        \n",
    "        self.decoder = nn.Linear(2*hidden_dim, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, X_batch, num_sents, sent_lens, return_attn_weights=False):\n",
    "        max_sent_len = X_batch.shape[2]\n",
    "        max_num_sent = X_batch.shape[1]\n",
    "        \n",
    "        # Use word embeddings to form sentence embeddings\n",
    "        word_attn_weights = []\n",
    "        docs = []\n",
    "        for doc, n, lens in zip(X_batch, num_sents, sent_lens):\n",
    "            words_batch = doc[:n]\n",
    "            embeddings = self.embedding(words_batch)\n",
    "            output, (_, _) = self.word_encoder(embeddings)\n",
    "            padding_positions = self.__get_padding_masks(lens[:n], max_sent_len).to(output.device)\n",
    "            sent_embeddings = self.word_attn(output, padding_positions=padding_positions, return_weights=return_attn_weights)\n",
    "            if return_attn_weights:\n",
    "                word_attn_weights.append(sent_embeddings[1])\n",
    "                sent_embeddings = sent_embeddings[0]\n",
    "            sent_embeddings = self.__repad_sentence_embeddings(sent_embeddings, max_num_sent)\n",
    "            docs.append(sent_embeddings)\n",
    "        \n",
    "        # Use sentence embeddings to form document embedding\n",
    "        sent_embeddings_batch = torch.stack(docs) \n",
    "        output, (_, _) = self.sent_encoder(sent_embeddings_batch)\n",
    "        padding_positions = self.__get_padding_masks(num_sents, max_num_sent).to(output.device)\n",
    "        doc_embeddings = self.word_attn(output, padding_positions=padding_positions, return_weights=return_attn_weights)\n",
    "        # Pass document embedding through output layer\n",
    "        if return_attn_weights:\n",
    "            return self.decoder(doc_embeddings[0]), word_attn_weights, doc_embeddings[1]\n",
    "        else:\n",
    "            return self.decoder(doc_embeddings)\n",
    "        \n",
    "    def __repad_sentence_embeddings(self, sents, max_num_sent):\n",
    "        return torch.cat([sents,\n",
    "                          torch.zeros((max_num_sent-sents.shape[0], \n",
    "                                       sents.shape[1]), device=sents.device)],dim=0)\n",
    "    \n",
    "    def __get_padding_masks(self, lengths, max_len):\n",
    "        '''\n",
    "        Returns a mask (shape BxLx1) that indicates the position of pad tokens as '1's\n",
    "        '''\n",
    "        return torch.tensor([[False]*i + [True]*(max_len-i) for i in lengths]).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below runs the preprocessing and saves the preprocessed items to the specified `SAVED_FOLDER_PATH`. As the preprocessing takes quite long, users are advised to skip this cell and use the already preprocessed inputs. Note:\n",
    "\n",
    "- `X` and `X_test` are numpy arrays of size `[NUM_DOCS, MAX_NUM_SENTS, MAX_SENT_LEN]`. Each entry is the corresponding token's index in the preprocessor's vocabulary. The (i,j,k)-th entry corresponds to the k-th token of the j-th sentence of the i-th document. \n",
    "- `ylens` and `ylens_test` are dataframes with `NUM_DOCS` rows and 3 columns: `Label`, `Num_Sentences` and `Num_Tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T18:37:58.210096Z",
     "iopub.status.busy": "2024-04-17T18:37:58.209522Z",
     "iopub.status.idle": "2024-04-17T18:38:02.441369Z",
     "shell.execute_reply": "2024-04-17T18:38:02.439836Z",
     "shell.execute_reply.started": "2024-04-17T18:37:58.210063Z"
    }
   },
   "outputs": [],
   "source": [
    "###### Uncomment this cell to run data preprocessing for HAN ############\n",
    "# SAVE_FOLDER_PATH = './HAN_prepro_data/'\n",
    "# FULL_TRAIN_DATA_PATH = '/kaggle/input/lun-glove/fulltrain.csv'\n",
    "# FULL_TEST_DATA_PATH = '/kaggle/input/lun-glove/balancedtest.csv'\n",
    "# GLOVE_TEXT_FILE_PATH = '/kaggle/input/lun-glove/glove.6B.100d.txt'\n",
    "\n",
    "# MAX_SENT_LEN = 30\n",
    "# MAX_NUM_SENTS = 30\n",
    "# NUM_CLASSES = 4\n",
    "# EMBED_DIM = 100\n",
    "\n",
    "# glovepp, embeds = DataPreprocessorHcl.from_pretrained_embeds(NUM_CLASSES, GLOVE_TEXT_FILE_PATH, EMBED_DIM)\n",
    "# train_df = pd.read_csv(FULL_TRAIN_DATA_PATH, header=None, names=['Label', 'Text'])\n",
    "# test_df = pd.read_csv(FULL_TEST_DATA_PATH, header=None, names=['Label', 'Text'])\n",
    "# X, ylens = glovepp.preprocess_data(train_df, MAX_SENT_LEN, MAX_NUM_SENTS)\n",
    "# X_test, ylens_test = glovepp.preprocess_data(test_df, MAX_SENT_LEN, MAX_NUM_SENTS)\n",
    "\n",
    "# VOCAB_LEN = len(glovepp.vocab)\n",
    "\n",
    "# np.save(SAVE_FOLDER_PATH+'X_train_prep.npy',X)\n",
    "# np.save(SAVE_FOLDER_PATH+'X_test_prep.npy',X_test)\n",
    "# ylens.to_csv(SAVE_FOLDER_PATH+'ylens_train_prep.csv', index=False)\n",
    "# ylens_test.to_csv(SAVE_FOLDER_PATH+'ylens_test_prep.csv', index=False)\n",
    "# np.save(SAVE_FOLDER_PATH+'glove_embs.npy',embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T03:30:55.024946Z",
     "iopub.status.busy": "2024-04-18T03:30:55.024221Z",
     "iopub.status.idle": "2024-04-18T03:31:02.996773Z",
     "shell.execute_reply": "2024-04-18T03:31:02.995667Z",
     "shell.execute_reply.started": "2024-04-18T03:30:55.024914Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 30\n",
    "MAX_NUM_SENTS = 30\n",
    "NUM_CLASSES = 4\n",
    "EMBED_DIM = 100\n",
    "VOCAB_SIZE = 400001 #hardcoded for convenience; see prev cell for how it was obtained\n",
    "\n",
    "X = np.load('./HAN_prepro_data/X_train_prep.npy')\n",
    "ylens = pd.read_csv('./HAN_prepro_data/ylens_train_prep.csv')\n",
    "X_test = np.load('./HAN_prepro_data/X_test_prep.npy')\n",
    "ylens_test = pd.read_csv('./HAN_prepro_data/ylens_test_prep.csv')\n",
    "embeds = torch.tensor(np.load('./glove_embs.npy'))\n",
    "\n",
    "import ast\n",
    "ylens['Num_Tokens'] = ylens['Num_Tokens'].apply(ast.literal_eval)\n",
    "ylens_test['Num_Tokens'] = ylens_test['Num_Tokens'].apply(ast.literal_eval)\n",
    "\n",
    "X_train, X_val, ylens_train, ylens_val = train_test_split(X, ylens, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T03:31:02.998465Z",
     "iopub.status.busy": "2024-04-18T03:31:02.998164Z",
     "iopub.status.idle": "2024-04-18T03:31:03.152163Z",
     "shell.execute_reply": "2024-04-18T03:31:03.151024Z",
     "shell.execute_reply.started": "2024-04-18T03:31:02.998441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes correct\n",
      "Num sentences correct\n",
      "Num tokenss correct\n",
      "Num classes correct\n",
      "Num sentences correct\n",
      "Num tokenss correct\n"
     ]
    }
   ],
   "source": [
    "def check_for_bugs(ylens, num_classes, max_num_sent, max_sent_len):\n",
    "    if (ylens['Label'] < num_classes).all():\n",
    "        print(\"Num classes correct\")\n",
    "    else:\n",
    "        assert False\n",
    "    if (ylens['Num_Sentences'] <= max_num_sent).all():\n",
    "        print(\"Num sentences correct\")\n",
    "    else:\n",
    "        assert False\n",
    "    if (ylens['Num_Tokens'].apply(lambda ls : all([le <= max_sent_len for le in ls]))).all():\n",
    "        print(\"Num tokenss correct\")\n",
    "    else:\n",
    "        assert False\n",
    "    return\n",
    "check_for_bugs(ylens, NUM_CLASSES, MAX_NUM_SENTS, MAX_SENT_LEN)\n",
    "check_for_bugs(ylens_test, NUM_CLASSES, MAX_NUM_SENTS, MAX_SENT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T06:39:42.922148Z",
     "iopub.status.busy": "2024-04-16T06:39:42.921745Z",
     "iopub.status.idle": "2024-04-16T06:39:42.943803Z",
     "shell.execute_reply": "2024-04-16T06:39:42.942866Z",
     "shell.execute_reply.started": "2024-04-16T06:39:42.922117Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 256\n",
    "VALID_BATCH_SIZE = 512\n",
    "train_loader = DataLoader(WrapperDatasetHcl(X_train, ylens_train),\n",
    "                          batch_size=TRAIN_BATCH_SIZE,\n",
    "                          collate_fn=collate_fnHcl,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(WrapperDatasetHcl(X_val, ylens_val),\n",
    "                          batch_size=VALID_BATCH_SIZE,\n",
    "                          collate_fn=collate_fnHcl,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(WrapperDatasetHcl(X_test, ylens_test),\n",
    "                          batch_size=VALID_BATCH_SIZE,\n",
    "                          collate_fn=collate_fnHcl,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T13:38:01.405250Z",
     "iopub.status.busy": "2024-04-09T13:38:01.404861Z",
     "iopub.status.idle": "2024-04-09T14:56:23.521892Z",
     "shell.execute_reply": "2024-04-09T14:56:23.520888Z",
     "shell.execute_reply.started": "2024-04-09T13:38:01.405221Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "HIDDEN_DIM = 100\n",
    "NUM_LSTM_LAYERS = 1\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 5e-04\n",
    "WEIGHT_DECAY = 5e-06\n",
    "LR_ANNEAL_FACTOR = 0.5\n",
    "LR_ANNEAL_PATIENCE = 2\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SAVE_FOLDER_PATH=\"./outputs/\"\n",
    "FILES_NAME_FORMAT=\"bestHAN_msl{}_mns{}_ba{}_emb{}hid{}lay{}cla{}_ep{}lr{}wd{}_af{}ap{}\"\n",
    "\n",
    "records = {\"path\": [], \"precision\":[], \"recall\": [], \"f1\": [], \"acc\":[]}\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = BiLSTMHeAttFCNNClassifier(VOCAB_SIZE,\n",
    "                              EMBED_DIM,\n",
    "                              HIDDEN_DIM,\n",
    "                              NUM_LSTM_LAYERS,\n",
    "                              NUM_CLASSES,\n",
    "                              pretrained_embeddings = embeds.to(torch.float32))\n",
    "trainer = Trainer(model, train_loader, val_loader,\n",
    "                  NUM_EPOCHS, LEARNING_RATE,\n",
    "                  weight_decay=WEIGHT_DECAY,\n",
    "                  lr_anneal_factor=LR_ANNEAL_FACTOR,\n",
    "                  lr_anneal_patience=LR_ANNEAL_PATIENCE\n",
    "                  save_loss_acc_plots=True)\n",
    "model_path, _ = trainer.train(\n",
    "    SAVE_FOLDER_PATH,\n",
    "    FILES_NAME_FORMAT.format(MAX_SENT_LEN,MAX_NUM_SENTS, TRAIN_BATCH_SIZE, EMBED_DIM,\n",
    "                             HIDDEN_DIM, NUM_LSTM_LAYERS, NUM_CLASSES,\n",
    "                             NUM_EPOCHS, LEARNING_RATE, WEIGHT_DECAY,\n",
    "                             LR_ANNEAL_FACTOR, LR_ANNEAL_PATIENCE),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T06:39:49.357490Z",
     "iopub.status.busy": "2024-04-16T06:39:49.356848Z",
     "iopub.status.idle": "2024-04-16T06:40:15.774810Z",
     "shell.execute_reply": "2024-04-16T06:40:15.773628Z",
     "shell.execute_reply.started": "2024-04-16T06:39:49.357449Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33/2069489799.py:100: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  X = torch.tensor(X, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.78      0.85       750\n",
      "           1       0.77      0.54      0.64       750\n",
      "           2       0.64      0.79      0.71       750\n",
      "           3       0.75      0.92      0.83       750\n",
      "\n",
      "    accuracy                           0.76      3000\n",
      "   macro avg       0.77      0.76      0.76      3000\n",
      "weighted avg       0.77      0.76      0.76      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "MODEL_PATH='./outputs/model/bestHAN_msl30_mns30_ba256_emb100hid100lay1cla4_ep10lr0.0005wd5e-06_af0.5_ap2_model'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = BiLSTMHeAttFCNNClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LSTM_LAYERS,\n",
    "                                  NUM_CLASSES, pretrained_embeddings = embeds.to(torch.float32))\n",
    "model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "\n",
    "records = {'split':[], 'acc':[],'f1':[],'precision':[], 'recall':[]}\n",
    "for i, loader in enumerate([train_loader, val_loader, test_loader]):\n",
    "    \n",
    "        records['split'].append('train' if i == 0 else 'val' if i==1 else 'test')\n",
    "        \n",
    "        preds = []\n",
    "        truths = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y, num_sent, sent_len in loader:\n",
    "                #Move to correct device\n",
    "                X = X.to(DEVICE)\n",
    "\n",
    "                #Forward pass\n",
    "                outputs = model(X, num_sent, sent_len)\n",
    "\n",
    "                #Logging\n",
    "                preds.append(torch.argmax(outputs, dim=-1).cpu())\n",
    "                truths.append(y)\n",
    "        preds = torch.cat(preds)\n",
    "        truths = torch.cat(truths)\n",
    "        records['acc'].append(accuracy_score(truths, preds))\n",
    "        records['f1'].append(f1_score(truths, preds, average='macro'))\n",
    "        records['precision'].append(precision_score(truths, preds, average='macro'))\n",
    "        records['recall'].append(recall_score(truths, preds, average='macro'))\n",
    "        if i == 0:\n",
    "            print(\"TRAINING DATA\")\n",
    "        elif i == 1:\n",
    "            print(\"VALIDATION DATA\")\n",
    "        elif i == 2:\n",
    "            print(\"TEST DATA\")\n",
    "        print(classification_report(truths, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Flat Attention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Related Classes and Functions; Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T06:35:38.804920Z",
     "iopub.status.busy": "2024-04-17T06:35:38.804194Z",
     "iopub.status.idle": "2024-04-17T06:35:38.881910Z",
     "shell.execute_reply": "2024-04-17T06:35:38.880392Z",
     "shell.execute_reply.started": "2024-04-17T06:35:38.804883Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataPreprocessorFlat():\n",
    "\n",
    "    def __init__(self, num_classes, data_vocab):\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab = data_vocab\n",
    "        print(\"Vocab created: {} unique tokens\".format(len(self.vocab)))\n",
    "        \n",
    "    @classmethod\n",
    "    def from_train_df(cls, num_classes, train_df, specials=['<unk>']):\n",
    "        data_vocab = build_vocab_from_iterator(cls.__yield_tokens(train_df), specials=specials)\n",
    "        data_vocab.set_default_index(data_vocab['<unk>'])\n",
    "        return cls(num_classes, data_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained_embeds(cls, num_classes, embed_path, embed_dim, sep=\" \",  specials=['<unk>']):\n",
    "        # start with all '0's for special tokens\n",
    "        embeds = [np.asarray([0]*embed_dim, dtype=np.float32)]*len(specials)\n",
    "        words = OrderedDict()\n",
    "        with open(embed_path, encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == 38522 and 'twitter.27B.100d' in embed_path:\n",
    "                    continue\n",
    "                splitline = line.split()\n",
    "                \n",
    "                word = splitline[0]\n",
    "                words[word] = 1\n",
    "                \n",
    "                embeds.append(np.asarray(splitline[1:], dtype=np.float32))\n",
    "                \n",
    "        embeds = torch.tensor(np.array(embeds))\n",
    "        data_vocab = vocab(words, min_freq=1, specials=specials)\n",
    "        data_vocab.set_default_index(data_vocab['<unk>'])\n",
    "        return cls(num_classes, data_vocab), embeds\n",
    "\n",
    "    @classmethod\n",
    "    def __yield_tokens(cls, df):\n",
    "        for row in df.itertuples(index=False):\n",
    "            yield word_tokenize(row.Text.replace(\"'\",\"\").lower())\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def preprocess_data_flat(self, df, clean=True):\n",
    "        '''\n",
    "        Converts text into integers that index the vocab,\n",
    "        and converts labels into the range [0,num_classes-1]\n",
    "        '''\n",
    "        if clean:\n",
    "            X = df['Text'].apply(lambda t: [self.vocab(word_tokenize(s.lower())) for s in sent_tokenize(t.replace(\"'\",\"\"))])\n",
    "            X = X.apply(lambda ls_of_ls: [token for ls in ls_of_ls for token in ls])\n",
    "        else:\n",
    "            X = df['Text'].apply(lambda t: self.vocab(word_tokenize(t)))\n",
    "\n",
    "        if self.num_classes == 4:\n",
    "            y = df['Label'].apply(lambda l: l-1)\n",
    "        else: #num_classes == 2\n",
    "            # assume test.xlsx\n",
    "            y = df['Label']\n",
    "        return X, y\n",
    "    \n",
    "class WrapperDatasetFlat(Dataset):\n",
    "    '''\n",
    "    Wrapper for use with dataloader\n",
    "    '''\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X.iloc[idx], self.y.iloc[idx]\n",
    "                 \n",
    "def make_flat_collate_function(model_max_len):\n",
    "    '''\n",
    "    Returns a function that pads and truncates sequences\n",
    "    in each batch up to min(batch's max length, model_max_len).\n",
    "    '''\n",
    "    def collate(batch):\n",
    "        '''\n",
    "        Returns the batch of padded sequences\n",
    "        and corresponding lengths and labels\n",
    "        '''\n",
    "        batch_max_len = max([len(tids) for tids, _ in batch])\n",
    "        output_len = min(model_max_len, batch_max_len)\n",
    "\n",
    "        X_padded = torch.zeros((len(batch), output_len), dtype=torch.long)\n",
    "        lengths = torch.empty((len(batch)), dtype=torch.long)\n",
    "        labels = torch.empty((len(batch)), dtype=torch.long)\n",
    "        for i, (tids, label) in enumerate(batch):\n",
    "            if len(tids) > output_len:\n",
    "                # sequence longer than output_len --> truncate\n",
    "                X_padded[i, :] = torch.tensor(tids[:output_len], dtype=torch.long)\n",
    "                lengths[i] = output_len\n",
    "            else:\n",
    "                # sequence shorter than output_len --> pad\n",
    "                X_padded[i, :len(tids)] = torch.tensor(tids, dtype=torch.long)\n",
    "                lengths[i] = len(tids)\n",
    "            labels[i] = label\n",
    "        return X_padded, lengths, labels\n",
    "                 \n",
    "    return collate\n",
    "\n",
    "class FlatTrainer():\n",
    "    def __init__(self, model, train_loader, val_loader, num_epochs, lr, weight_decay = 0,\n",
    "               lr_anneal_factor=None, lr_anneal_patience=None,\n",
    "               loss_weights=None,\n",
    "               save_loss_acc_plots=True):\n",
    "        #Data\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        #Model\n",
    "        self.model=model.to(DEVICE)\n",
    "\n",
    "        #Training\n",
    "        self.num_epochs = num_epochs\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        if loss_weights is not None:\n",
    "            self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(loss_weights, dtype=torch.float32, device=DEVICE))\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.to_anneal_lr = lr_anneal_factor and lr_anneal_patience\n",
    "        if self.to_anneal_lr:\n",
    "            self.scheduler = ReduceLROnPlateau(self.optimizer,\n",
    "                                             factor=lr_anneal_factor,\n",
    "                                             patience=lr_anneal_patience)\n",
    "\n",
    "    def train(self, save_folder_root_path, file_name_root,\n",
    "            plot_loss_acc=True, verbose=True):\n",
    "        #account for pytorch versions < 2.2\n",
    "        if hasattr(self.scheduler, 'verbose'):\n",
    "            self.scheduler.verbose=verbose\n",
    "            \n",
    "        #Set up logging\n",
    "        self.model_folder = save_folder_root_path+'model/'\n",
    "        self.plots_folder = save_folder_root_path+'plots/'\n",
    "        self.file_name_root = file_name_root\n",
    "        self.plot_loss_acc = plot_loss_acc\n",
    "        if not os.path.exists(self.model_folder):\n",
    "            os.makedirs(self.model_folder)\n",
    "        if self.plot_loss_acc and not os.path.exists(self.plots_folder):\n",
    "            os.makedirs(self.plots_folder)\n",
    "        self.train_metrics = {'loss':[], 'accuracy':[], 'f1':[]}\n",
    "        self.val_metrics = {'loss':[], 'accuracy':[], 'f1':[]}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(1, self.num_epochs+1):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            preds = []\n",
    "            truths = []\n",
    "            for X, lengths, y in tqdm(self.train_loader, disable = not verbose):\n",
    "                #Move to correct device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                #Forward pass\n",
    "                outputs = self.model(X, lengths)\n",
    "                if type(outputs)==tuple:\n",
    "                    logits = outputs[0]\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                loss = self.loss_fn(logits, y)\n",
    "\n",
    "                #Backprop\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                #Logging\n",
    "                train_loss.append(loss.item())\n",
    "                preds.append(torch.argmax(logits, dim=-1).cpu())\n",
    "                truths.append(y.cpu())\n",
    "\n",
    "            #Validation\n",
    "            self.model.eval()\n",
    "            val_loss, val_acc, val_f1 = self.validate()\n",
    "\n",
    "            #Logging\n",
    "            train_loss = sum(train_loss)/len(train_loss)\n",
    "            preds = torch.cat(preds)\n",
    "            truths = torch.cat(truths)\n",
    "            train_acc = accuracy_score(truths, preds)\n",
    "            train_f1 = f1_score(truths, preds, average='macro')\n",
    "\n",
    "            self.train_metrics['loss'].append(train_loss)\n",
    "            self.train_metrics['accuracy'].append(train_acc)\n",
    "            self.train_metrics['f1'].append(train_f1)\n",
    "            self.val_metrics['loss'].append(val_loss)\n",
    "            self.val_metrics['accuracy'].append(val_acc)\n",
    "            self.val_metrics['f1'].append(val_f1)\n",
    "            \n",
    "            if verbose:\n",
    "                tqdm.write(\"Epoch {} Complete:\\n Train: loss={}, acc={}, F1={}\\n Val  : loss={}, acc={}, F1={}\\n\".format(\n",
    "                            i, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1))\n",
    "            #Save model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.model.state_dict(),\n",
    "                           self.model_folder + file_name_root + '_model.pt')\n",
    "            \n",
    "            ##LR Annealing\n",
    "            if self.to_anneal_lr:\n",
    "                #account for pytorch versions >= 2.2\n",
    "                if not hasattr(self.scheduler, 'verbose') and i > 1:\n",
    "                    last_lr = self.scheduler.get_last_lr()\n",
    "                    \n",
    "                self.scheduler.step(val_loss)\n",
    "                \n",
    "                #account for pytorch versions >= 2.2\n",
    "                if not hasattr(self.scheduler, 'verbose') and i > 1 and last_lr != self.scheduler.get_last_lr():\n",
    "                    print(\"LR Reduced from {} to {} for next epoch onwards\".format(last_lr, self.scheduler.get_last_lr()))\n",
    "                    \n",
    "        #Save plots\n",
    "        if self.plot_loss_acc:\n",
    "            self.plot_metrics(self.train_metrics, self.val_metrics)\n",
    "\n",
    "        return self.model_folder + file_name_root + '_model.pt', self.model_folder + file_name_root + 'plot.pt'\n",
    "\n",
    "    def validate(self, test_loader=None):\n",
    "        with torch.no_grad():\n",
    "            losses = []\n",
    "            preds = []\n",
    "            truths = []\n",
    "            val_loader = self.val_loader if test_loader is None else test_loader\n",
    "            for X, lengths, y in val_loader:\n",
    "                #Move to correct device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                #Forward pass\n",
    "                outputs = self.model(X, lengths)\n",
    "                \n",
    "                if type(outputs)==tuple:\n",
    "                    logits = outputs[0]\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                loss = self.loss_fn(logits, y)\n",
    "\n",
    "                #Logging\n",
    "                losses.append(loss.item())\n",
    "                preds.append(torch.argmax(logits, dim=-1).cpu())\n",
    "                truths.append(y.cpu())\n",
    "        preds = torch.cat(preds)\n",
    "        truths = torch.cat(truths)\n",
    "        return sum(losses)/len(losses), accuracy_score(truths, preds), f1_score(truths, preds, average='macro')\n",
    "\n",
    "    def plot_metrics(self, train_metrics, val_metrics):\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "        axs[0].plot(range(1, self.num_epochs + 1), train_metrics['loss'], color='b', label='Train')\n",
    "        axs[0].plot(range(1, self.num_epochs + 1), val_metrics['loss'], color='r', label='Validation')\n",
    "        axs[0].set_ylabel('Loss')\n",
    "        axs[0].set_xlabel('Epochs')\n",
    "        axs[0].set_ylim(bottom=0)\n",
    "        axs[0].set_xticks(range(0, self.num_epochs + 1, 2))\n",
    "        axs[0].grid(visible=True, which='major', axis='both')\n",
    "\n",
    "        axs[1].plot(range(1, self.num_epochs + 1), train_metrics['accuracy'], color='b', label='Train')\n",
    "        axs[1].plot(range(1, self.num_epochs + 1), val_metrics['accuracy'], color='r', label='Validation')\n",
    "        axs[1].set_ylabel('Accuracy')\n",
    "        axs[1].set_xlabel('Epochs')\n",
    "        axs[1].set_ylim(bottom=0)\n",
    "        axs[1].set_xticks(range(0, self.num_epochs + 1, 2))\n",
    "        axs[1].grid(visible=True, which='major', axis='both')\n",
    "\n",
    "        axs[2].plot(range(1, self.num_epochs + 1), train_metrics['f1'], color='b', label='Train')\n",
    "        axs[2].plot(range(1, self.num_epochs + 1), val_metrics['f1'], color='r', label='Validation')\n",
    "        axs[2].set_ylabel('F1 (Macro-averaged)')\n",
    "        axs[2].set_xlabel('Epochs')\n",
    "        axs[2].set_ylim(bottom=0)\n",
    "        axs[2].set_xticks(range(0, self.num_epochs + 1, 2))\n",
    "        axs[2].grid(visible=True, which='major', axis='both')\n",
    "\n",
    "        fig.legend(*axs[2].get_legend_handles_labels(), loc='upper center')\n",
    "        fig.savefig(self.plots_folder + self.file_name_root + '_plot.png')\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T06:27:38.102690Z",
     "iopub.status.busy": "2024-04-16T06:27:38.102256Z",
     "iopub.status.idle": "2024-04-16T06:27:38.122977Z",
     "shell.execute_reply": "2024-04-16T06:27:38.121655Z",
     "shell.execute_reply.started": "2024-04-16T06:27:38.102656Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionUnit(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=None, num_outputs=1, attn_dropout=0.0):\n",
    "        super(AttentionUnit, self).__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = input_dim\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.query = nn.Linear(hidden_dim, num_outputs, bias=False)\n",
    "    def forward(self, encoder_output, padding_positions=None, return_weights=False):\n",
    "        # [B,L,H]-->[B,L,H]\n",
    "        hidden_rep = F.tanh(self.hidden(encoder_output))\n",
    "        # [B,L,H]-->[B,L,1]\n",
    "        similarity = self.query(hidden_rep)\n",
    "        if padding_positions is not None:\n",
    "            similarity = similarity.masked_fill(padding_positions, -float('inf'))\n",
    "        attention_weights = F.softmax(similarity, dim=1)\n",
    "        #Return weighted sum [B,L,1], [B,L,H]-->[B,H]\n",
    "        if return_weights:\n",
    "            return torch.bmm(attention_weights.transpose(1,2), hidden_rep).squeeze(1), attention_weights\n",
    "        return torch.bmm(attention_weights.transpose(1,2), hidden_rep).squeeze(1)\n",
    "\n",
    "class LSTMFlatAttentionFCNNClassifier(torch.nn.Module):\n",
    "    '''\n",
    "    Classifier that uses an LSTM as an encoder followed by an attention block\n",
    "    and a Fully-Connected Neural Network(FCNN) as a decoder.\n",
    "    '''\n",
    "    def __init__(self, vocab_len, embed_dim, hidden_dim, num_lstm_layers, num_classes, attn_dropout=0.0, pretrained_embeddings=None, freeze_embeds=False):\n",
    "        super(LSTMFlatAttentionFCNNClassifier, self).__init__()\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=freeze_embeds)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_len, embedding_dim=embed_dim)\n",
    "\n",
    "        self.encoder = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=num_lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.attn = AttentionUnit(2*hidden_dim)\n",
    "        self.decoder = nn.Linear(2*hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, X_batch, lengths, return_attn_weights=False):\n",
    "        embeddings = self.embedding(X_batch)\n",
    "\n",
    "        embeddings = nn.utils.rnn.pack_padded_sequence(embeddings, lengths.cpu(), enforce_sorted=False, batch_first=True)\n",
    "        output, (_, _) = self.encoder(embeddings)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output,batch_first=True)\n",
    "\n",
    "        padding_positions = self.__get_padding_masks(lengths).to(output.device)\n",
    "        doc_embeddings = self.attn(output,padding_positions=padding_positions,return_weights=return_attn_weights)\n",
    "        \n",
    "        if return_attn_weights:\n",
    "            return self.decoder(doc_embeddings[0]), doc_embeddings[1]\n",
    "        else:\n",
    "            return self.decoder(doc_embeddings)\n",
    "    \n",
    "    def __get_padding_masks(self, lengths):\n",
    "        '''\n",
    "        Returns a mask (shape BxLx1) that indicates the position of pad tokens\n",
    "        '''\n",
    "        max_len = lengths.max()\n",
    "        return torch.tensor([[False]*i + [True]*(max_len-i) for i in lengths]).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:11:34.250908Z",
     "iopub.status.busy": "2024-04-10T07:11:34.250646Z",
     "iopub.status.idle": "2024-04-10T07:11:35.712153Z",
     "shell.execute_reply": "2024-04-10T07:11:35.711121Z",
     "shell.execute_reply.started": "2024-04-10T07:11:34.250885Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('/kaggle/input/lun-glove/fulltrain.csv', header=None, names=['Label','Text'])\n",
    "# test_df = pd.read_excel('/kaggle/input/lun-glove/balancedtest.csv', header=None, names=['Label','Text'])\n",
    "\n",
    "# dp, embeds = DataPreprocessorFlat.from_pretrained_embeds(4, '/kaggle/input/lun-glove/glove.6B.100d.txt', 100, sep=\" \",  specials=['<unk>'])\n",
    "# X, y = dp.preprocess_data_flat(train_df, clean=True)\n",
    "# X_test, y_test = dp.preprocess_data_flat(test_df, clean=True)\n",
    "# X.to_parquet('./FAN_prepro_data/X_train_prep_flat.parquet')\n",
    "# y.to_parquet('./FAN_prepro_data/y_train_prep_flat.parquet')\n",
    "# X_test.to_parquet('./FAN_prepro_data/X_test_prep_flat.parquet')\n",
    "# y_test.to_parquet('./FAN_prepro_data/y_test_prep_flat.parquet')\n",
    "\n",
    "# VOCAB_LEN = len(dp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T06:27:41.805539Z",
     "iopub.status.busy": "2024-04-16T06:27:41.804614Z",
     "iopub.status.idle": "2024-04-16T06:27:45.992195Z",
     "shell.execute_reply": "2024-04-16T06:27:45.990980Z",
     "shell.execute_reply.started": "2024-04-16T06:27:41.805504Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.read_parquet('./FAN_prepro_data/X_train_prep_flat.parquet')['Text']\n",
    "y = pd.read_parquet('./FAN_prepro_data/y_train_prep_flat.parquet')['Label']\n",
    "X_test = pd.read_parquet('./FAN_prepro_data/X_test_prep_flat.parquet')['Text']\n",
    "y_test = pd.read_parquet('./FAN_prepro_data/y_test_prep_flat.parquet')['Label']\n",
    "embeds = torch.tensor(np.load('./glove_embs.npy'))\n",
    "VOCAB_SIZE = 400001\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T06:27:45.994720Z",
     "iopub.status.busy": "2024-04-16T06:27:45.994363Z",
     "iopub.status.idle": "2024-04-16T06:27:46.002860Z",
     "shell.execute_reply": "2024-04-16T06:27:46.001962Z",
     "shell.execute_reply.started": "2024-04-16T06:27:45.994691Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VALID_BATCH_SIZE = 512\n",
    "MODEL_MAX_LEN = 500\n",
    "collate_flat = make_flat_collate_function(MODEL_MAX_LEN)\n",
    "train_loader = DataLoader(WrapperDatasetFlat(X_train, y_train),\n",
    "                          batch_size=TRAIN_BATCH_SIZE,\n",
    "                          collate_fn=collate_flat,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(WrapperDatasetFlat(X_val, y_val),\n",
    "                          batch_size=VALID_BATCH_SIZE,\n",
    "                          collate_fn=collate_flat,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(WrapperDatasetFlat(X_test, y_test),\n",
    "                          batch_size=VALID_BATCH_SIZE,\n",
    "                          collate_fn=collate_flat,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T16:24:41.686091Z",
     "iopub.status.busy": "2024-04-14T16:24:41.685706Z",
     "iopub.status.idle": "2024-04-14T16:46:47.380480Z",
     "shell.execute_reply": "2024-04-14T16:46:47.379439Z",
     "shell.execute_reply.started": "2024-04-14T16:24:41.686060Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "NUM_LSTM_LAYERS = 1\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 5e-04\n",
    "WEIGHT_DECAY = 5e-06\n",
    "LR_ANNEAL_FACTOR = 0.5\n",
    "LR_ANNEAL_PATIENCE = 2\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SAVE_FOLDER_PATH=\"./outputs/\"\n",
    "FILES_NAME_FORMAT=\"bestFAN_ml{}_ba{}_emb{}hid{}lay{}cla{}_ep{}lr{}wd{}_af{}ap{}\"\n",
    "records = {\"path\": [], \"precision\":[], \"recall\": [], \"f1\": [], \"acc\":[]}\n",
    "\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = LSTMFlatAttentionFCNNClassifier(VOCAB_SIZE,\n",
    "                              EMBED_DIM,\n",
    "                              HIDDEN_DIM,\n",
    "                              NUM_LSTM_LAYERS,\n",
    "                              NUM_CLASSES,\n",
    "                              pretrained_embeddings = embeds.to(torch.float32))\n",
    "trainer = FlatTrainer(model, train_loader, val_loader,\n",
    "                  NUM_EPOCHS, LEARNING_RATE,\n",
    "                  weight_decay=WEIGHT_DECAY,\n",
    "                  lr_anneal_factor=LR_ANNEAL_FACTOR,\n",
    "                  lr_anneal_patience=LR_ANNEAL_PATIENCE,\n",
    "                  loss_weights=LOSS_WEIGHTS,\n",
    "                  save_loss_acc_plots=True)\n",
    "model_path, _ = trainer.train(SAVE_FOLDER_PATH,\n",
    "              FILES_NAME_FORMAT.format(MODEL_MAX_LEN, TRAIN_BATCH_SIZE, EMBED_DIM,\n",
    "                                       HIDDEN_DIM, NUM_LSTM_LAYERS, NUM_CLASSES,\n",
    "                                      NUM_EPOCHS, LEARNING_RATE, WEIGHT_DECAY,\n",
    "                                      LR_ANNEAL_FACTOR, LR_ANNEAL_PATIENCE),\n",
    "              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T06:28:59.546937Z",
     "iopub.status.busy": "2024-04-16T06:28:59.546290Z",
     "iopub.status.idle": "2024-04-16T06:29:13.774799Z",
     "shell.execute_reply": "2024-04-16T06:29:13.773564Z",
     "shell.execute_reply.started": "2024-04-16T06:28:59.546884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81       750\n",
      "           1       0.78      0.46      0.58       750\n",
      "           2       0.58      0.72      0.64       750\n",
      "           3       0.67      0.93      0.78       750\n",
      "\n",
      "    accuracy                           0.71      3000\n",
      "   macro avg       0.74      0.71      0.70      3000\n",
      "weighted avg       0.74      0.71      0.70      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "base=\"./outputs/model/bestFAN_ml500_ba256_emb100hid100lay1cla4_ep10lr0.0005wd5e-06_af0.5ap2_model.pt\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "records = {'split':[], 'acc':[],'f1':[],'precision':[], 'recall':[]}\n",
    "for i, loader in enumerate([train_loader, val_loader, test_loader]):\n",
    "        model = LSTMFlatAttentionFCNNClassifier(VOCAB_SIZE, 100, 100, 1, 4,\n",
    "                                                pretrained_embeddings = embeds.to(torch.float32))\n",
    "        model.to(DEVICE)\n",
    "        model.load_state_dict(torch.load(base, map_location=DEVICE))\n",
    "        preds = []\n",
    "        truths = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, lengths, y in loader:\n",
    "                #Move to correct device\n",
    "                X = X.to(DEVICE)\n",
    "\n",
    "                #Forward pass\n",
    "                outputs = model(X, lengths)\n",
    "\n",
    "                #Logging\n",
    "                preds.append(torch.argmax(outputs, dim=-1).cpu())\n",
    "                truths.append(y)\n",
    "        preds = torch.cat(preds)\n",
    "        truths = torch.cat(truths)\n",
    "        records['split'].append('train' if i==0 else 'val' if i==1 else 'test')\n",
    "        records['acc'].append(accuracy_score(truths, preds))\n",
    "        records['f1'].append(f1_score(truths, preds, average='macro'))\n",
    "        records['precision'].append(precision_score(truths, preds, average='macro'))\n",
    "        records['recall'].append(recall_score(truths, preds, average='macro'))\n",
    "        if i == 0:\n",
    "            print(\"TRAINING DATA\")\n",
    "        elif i == 1:\n",
    "            print(\"VALIDATION DATA\")\n",
    "        elif i == 2:\n",
    "            print(\"TEST DATA\")\n",
    "        print(classification_report(truths, preds))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4749401,
     "sourceId": 8081439,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4747496,
     "sourceId": 8122822,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 23804,
     "sourceId": 28272,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 24461,
     "sourceId": 29043,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
